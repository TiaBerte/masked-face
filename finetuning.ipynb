{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/TiaBerte/masked-face/blob/main/finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "dhu-IvtmuU97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s git://github.com/TiaBerte/masked-face.git masked-face\n",
        "%cd masked-face/\n",
        "%ls"
      ],
      "metadata": {
        "id": "QW-Zz9n2uj2a",
        "outputId": "a9d0bfe0-9fb1-4b9d-f2f7-7e770b178285",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'masked-face'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 14538, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/14538)\u001b[K\rremote: Counting objects:   1% (146/14538)\u001b[K\rremote: Counting objects:   2% (291/14538)\u001b[K\rremote: Counting objects:   3% (437/14538)\u001b[K\rremote: Counting objects:   4% (582/14538)\u001b[K\rremote: Counting objects:   5% (727/14538)\u001b[K\rremote: Counting objects:   6% (873/14538)\u001b[K\rremote: Counting objects:   7% (1018/14538)\u001b[K\rremote: Counting objects:   8% (1164/14538)\u001b[K\rremote: Counting objects:   9% (1309/14538)\u001b[K\rremote: Counting objects:  10% (1454/14538)\u001b[K\rremote: Counting objects:  11% (1600/14538)\u001b[K\rremote: Counting objects:  12% (1745/14538)\u001b[K\rremote: Counting objects:  13% (1890/14538)\u001b[K\rremote: Counting objects:  14% (2036/14538)\u001b[K\rremote: Counting objects:  15% (2181/14538)\u001b[K\rremote: Counting objects:  16% (2327/14538)\u001b[K\rremote: Counting objects:  17% (2472/14538)\u001b[K\rremote: Counting objects:  18% (2617/14538)\u001b[K\rremote: Counting objects:  19% (2763/14538)\u001b[K\rremote: Counting objects:  20% (2908/14538)\u001b[K\rremote: Counting objects:  21% (3053/14538)\u001b[K\rremote: Counting objects:  22% (3199/14538)\u001b[K\rremote: Counting objects:  23% (3344/14538)\u001b[K\rremote: Counting objects:  24% (3490/14538)\u001b[K\rremote: Counting objects:  25% (3635/14538)\u001b[K\rremote: Counting objects:  26% (3780/14538)\u001b[K\rremote: Counting objects:  27% (3926/14538)\u001b[K\rremote: Counting objects:  28% (4071/14538)\u001b[K\rremote: Counting objects:  29% (4217/14538)\u001b[K\rremote: Counting objects:  30% (4362/14538)\u001b[K\rremote: Counting objects:  31% (4507/14538)\u001b[K\rremote: Counting objects:  32% (4653/14538)\u001b[K\rremote: Counting objects:  33% (4798/14538)\u001b[K\rremote: Counting objects:  34% (4943/14538)\u001b[K\rremote: Counting objects:  35% (5089/14538)\u001b[K\rremote: Counting objects:  36% (5234/14538)\u001b[K\rremote: Counting objects:  37% (5380/14538)\u001b[K\rremote: Counting objects:  38% (5525/14538)\u001b[K\rremote: Counting objects:  39% (5670/14538)\u001b[K\rremote: Counting objects:  40% (5816/14538)\u001b[K\rremote: Counting objects:  41% (5961/14538)\u001b[K\rremote: Counting objects:  42% (6106/14538)\u001b[K\rremote: Counting objects:  43% (6252/14538)\u001b[K\rremote: Counting objects:  44% (6397/14538)\u001b[K\rremote: Counting objects:  45% (6543/14538)\u001b[K\rremote: Counting objects:  46% (6688/14538)\u001b[K\rremote: Counting objects:  47% (6833/14538)\u001b[K\rremote: Counting objects:  48% (6979/14538)\u001b[K\rremote: Counting objects:  49% (7124/14538)\u001b[K\rremote: Counting objects:  50% (7269/14538)\u001b[K\rremote: Counting objects:  51% (7415/14538)\u001b[K\rremote: Counting objects:  52% (7560/14538)\u001b[K\rremote: Counting objects:  53% (7706/14538)\u001b[K\rremote: Counting objects:  54% (7851/14538)\u001b[K\rremote: Counting objects:  55% (7996/14538)\u001b[K\rremote: Counting objects:  56% (8142/14538)\u001b[K\rremote: Counting objects:  57% (8287/14538)\u001b[K\rremote: Counting objects:  58% (8433/14538)\u001b[K\rremote: Counting objects:  59% (8578/14538)\u001b[K\rremote: Counting objects:  60% (8723/14538)\u001b[K\rremote: Counting objects:  61% (8869/14538)\u001b[K\rremote: Counting objects:  62% (9014/14538)\u001b[K\rremote: Counting objects:  63% (9159/14538)\u001b[K\rremote: Counting objects:  64% (9305/14538)\u001b[K\rremote: Counting objects:  65% (9450/14538)\u001b[K\rremote: Counting objects:  66% (9596/14538)\u001b[K\rremote: Counting objects:  67% (9741/14538)\u001b[K\rremote: Counting objects:  68% (9886/14538)\u001b[K\rremote: Counting objects:  69% (10032/14538)\u001b[K\rremote: Counting objects:  70% (10177/14538)\u001b[K\rremote: Counting objects:  71% (10322/14538)\u001b[K\rremote: Counting objects:  72% (10468/14538)\u001b[K\rremote: Counting objects:  73% (10613/14538)\u001b[K\rremote: Counting objects:  74% (10759/14538)\u001b[K\rremote: Counting objects:  75% (10904/14538)\u001b[K\rremote: Counting objects:  76% (11049/14538)\u001b[K\rremote: Counting objects:  77% (11195/14538)\u001b[K\rremote: Counting objects:  78% (11340/14538)\u001b[K\rremote: Counting objects:  79% (11486/14538)\u001b[K\rremote: Counting objects:  80% (11631/14538)\u001b[K\rremote: Counting objects:  81% (11776/14538)\u001b[K\rremote: Counting objects:  82% (11922/14538)\u001b[K\rremote: Counting objects:  83% (12067/14538)\u001b[K\rremote: Counting objects:  84% (12212/14538)\u001b[K\rremote: Counting objects:  85% (12358/14538)\u001b[K\rremote: Counting objects:  86% (12503/14538)\u001b[K\rremote: Counting objects:  87% (12649/14538)\u001b[K\rremote: Counting objects:  88% (12794/14538)\u001b[K\rremote: Counting objects:  89% (12939/14538)\u001b[K\rremote: Counting objects:  90% (13085/14538)\u001b[K\rremote: Counting objects:  91% (13230/14538)\u001b[K\rremote: Counting objects:  92% (13375/14538)\u001b[K\rremote: Counting objects:  93% (13521/14538)\u001b[K\rremote: Counting objects:  94% (13666/14538)\u001b[K\rremote: Counting objects:  95% (13812/14538)\u001b[K\rremote: Counting objects:  96% (13957/14538)\u001b[K\rremote: Counting objects:  97% (14102/14538)\u001b[K\rremote: Counting objects:  98% (14248/14538)\u001b[K\rremote: Counting objects:  99% (14393/14538)\u001b[K\rremote: Counting objects: 100% (14538/14538)\u001b[K\rremote: Counting objects: 100% (14538/14538), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14530/14530), done.\u001b[K\n",
            "remote: Total 14538 (delta 12), reused 14526 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (14538/14538), 63.30 MiB | 35.00 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "Checking out files: 100% (12011/12011), done.\n",
            "/content/masked-face/masked-face\n",
            "barlowTwins.py  finetunevggface2.py  README.md    transformation.py\n",
            "\u001b[0m\u001b[01;34mdataset\u001b[0m/        finetuning.ipynb     resnet50.py  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7yNytZAuQ7T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from barlowTwins import BarlowTwins\n",
        "from utils import adjust_learning_rate, load_state_dict, get_mean_and_std, LARS\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQyi4-icuQ7Y"
      },
      "outputs": [],
      "source": [
        "\"\"\"Train = 2397\n",
        "val = 300\n",
        "Test = 300\n",
        "\"\"\"\n",
        "\n",
        "N_IDENTITY = 8631\n",
        "model = resnet50(num_classes=N_IDENTITY, include_top=True)#False)\n",
        "load_state_dict(model, PATH+'weights/resnet50_ft_weight.pkl')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device=device)\n",
        "\n",
        "\n",
        "\n",
        "train_path = PATH + 'dataset/train'\n",
        "val_path = PATH + 'dataset/val'\n",
        "test_path = PATH + 'dataset/test'\n",
        "\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(train_path, transform=transforms.ToTensor())\n",
        "\n",
        "dataset_mean = torch.Tensor([0.5360, 0.4703, 0.4324])\n",
        "dataset_std = torch.Tensor([0.2720, 0.2469, 0.2537])\n",
        "\n",
        "normalize = transforms.Normalize(dataset_mean, dataset_std)\n",
        "resize = transforms.Resize((244, 244))\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), normalize, resize])\n",
        "\n",
        "data_train = torchvision.datasets.ImageFolder(train_path, transform=transform)\n",
        "\n",
        "data_val = torchvision.datasets.ImageFolder(val_path, transform=transform)\n",
        "\n",
        "data_test = torchvision.datasets.ImageFolder(test_path, transform=transform)\n",
        "\n",
        "num_workers = 2\n",
        "size_batch_train = 64\n",
        "size_batch_val = 2 * size_batch_train\n",
        "\n",
        "loader_train = torch.utils.data.DataLoader(data_train, batch_size=size_batch_train, \n",
        "                                           shuffle=True, \n",
        "                                           pin_memory=True, \n",
        "                                           num_workers=num_workers)\n",
        "\n",
        "loader_val = torch.utils.data.DataLoader(data_val, batch_size=size_batch_val, \n",
        "                                         shuffle=False,\n",
        "                                         num_workers=num_workers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucerIxftuQ7a"
      },
      "source": [
        "**PARSER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n14f6iciuQ7c"
      },
      "outputs": [],
      "source": [
        "\n",
        "parser = argparse.ArgumentParser(description='Barlow Twins Training')\n",
        "parser.add_argument('data', type=Path, metavar='DIR',\n",
        "                    help='path to dataset')\n",
        "parser.add_argument('--workers', default=8, type=int, metavar='N',\n",
        "                    help='number of data loader workers')\n",
        "parser.add_argument('--epochs', default=1000, type=int, metavar='N',\n",
        "                    help='number of total epochs to run')\n",
        "parser.add_argument('--batch-size', default=2048, type=int, metavar='N',\n",
        "                    help='mini-batch size')\n",
        "parser.add_argument('--learning-rate-weights', default=0.2, type=float, metavar='LR',\n",
        "                    help='base learning rate for weights')\n",
        "parser.add_argument('--learning-rate-biases', default=0.0048, type=float, metavar='LR',\n",
        "                    help='base learning rate for biases and batch norm parameters')\n",
        "parser.add_argument('--weight-decay', default=1e-6, type=float, metavar='W',\n",
        "                    help='weight decay')\n",
        "parser.add_argument('--lambd', default=0.0051, type=float, metavar='L',\n",
        "                    help='weight on off-diagonal terms')\n",
        "parser.add_argument('--projector', default='8192-8192-8192', type=str,\n",
        "                    metavar='MLP', help='projector MLP')\n",
        "parser.add_argument('--print-freq', default=100, type=int, metavar='N',\n",
        "                    help='print frequency')\n",
        "parser.add_argument('--checkpoint-dir', default='./checkpoint/', type=Path,\n",
        "                    metavar='DIR', help='path to checkpoint directory')\n",
        "parser.add_argument('--backbone_lr', default=0, type=float, \n",
        "                    help='Learning rate used for fine tuning the backbone, disabled by default')\n",
        "\n",
        "\n",
        "\n",
        "def main_worker(gpu, args):\n",
        "    args.rank += gpu\n",
        "    torch.distributed.init_process_group(\n",
        "        backend='nccl', init_method=args.dist_url,\n",
        "        world_size=args.world_size, rank=args.rank)\n",
        "\n",
        "    if args.rank == 0:\n",
        "        args.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "        stats_file = open(args.checkpoint_dir / 'stats.txt', 'a', buffering=1)\n",
        "        print(' '.join(sys.argv))\n",
        "        print(' '.join(sys.argv), file=stats_file)\n",
        "\n",
        "    torch.cuda.set_device(gpu)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    model = BarlowTwins(args).cuda(gpu)\n",
        "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "    param_weights = []\n",
        "    param_biases = []\n",
        "    for param in model.parameters():\n",
        "        if param.ndim == 1:\n",
        "            param_biases.append(param)\n",
        "        else:\n",
        "            param_weights.append(param)\n",
        "\n",
        "    parameters = [{'params': param_weights}, {'params': param_biases}]\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
        "    optimizer = LARS(parameters, lr=0, weight_decay=args.weight_decay,\n",
        "                     weight_decay_filter=True,\n",
        "                     lars_adaptation_filter=True)\n",
        "\n",
        "    # automatically resume from checkpoint if it exists\n",
        "    if (args.checkpoint_dir / 'checkpoint.pth').is_file():\n",
        "        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',\n",
        "                          map_location='cpu')\n",
        "        start_epoch = ckpt['epoch']\n",
        "        model.load_state_dict(ckpt['model'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    #dataset = torchvision.datasets.ImageFolder(args.data / 'train', Transform())\n",
        "    data_train = torchvision.datasets.ImageFolder(train_path, transform=resize)\n",
        "    '''\n",
        "    Change the sampler\n",
        "    '''\n",
        "    sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
        "    assert args.batch_size % args.world_size == 0\n",
        "    per_device_batch_size = args.batch_size // args.world_size\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=per_device_batch_size, num_workers=args.workers,\n",
        "        pin_memory=True, sampler=sampler)\n",
        "\n",
        "    start_time = time.time()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        sampler.set_epoch(epoch)\n",
        "        for step, ((y1, y2), _) in enumerate(loader, start=epoch * len(loader)):\n",
        "            y1 = y1.cuda(gpu, non_blocking=True)\n",
        "            y2 = y2.cuda(gpu, non_blocking=True)\n",
        "            adjust_learning_rate(args, optimizer, loader, step)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                loss = model.forward(y1, y2)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            if step % args.print_freq == 0:\n",
        "                if args.rank == 0:\n",
        "                    stats = dict(epoch=epoch, step=step,\n",
        "                                 lr_weights=optimizer.param_groups[0]['lr'],\n",
        "                                 lr_biases=optimizer.param_groups[1]['lr'],\n",
        "                                 loss=loss.item(),\n",
        "                                 time=int(time.time() - start_time))\n",
        "                    print(json.dumps(stats))\n",
        "                    print(json.dumps(stats), file=stats_file)\n",
        "        if args.rank == 0:\n",
        "            # save checkpoint\n",
        "            state = dict(epoch=epoch + 1, model=model.state_dict(),\n",
        "                         optimizer=optimizer.state_dict())\n",
        "            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')\n",
        "    if args.rank == 0:\n",
        "        # save final model\n",
        "        torch.save(model.module.backbone.state_dict(),\n",
        "                   args.checkpoint_dir / 'resnet50.pth')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWsgh0RAuQ7f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "finetuning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}