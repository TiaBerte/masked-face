{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhu-IvtmuU97"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TiaBerte/masked-face/blob/main/finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "H7yNytZAuQ7T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from barlowTwins import BarlowTwins\n",
        "from utils import adjust_learning_rate, load_state_dict, get_mean_and_std, LARS\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataload import maskedFaceDataset\n",
        "import argparse\n",
        "\n",
        "path = 'C:/Users/Mattia/Documents/GitHub/masked-face/dataset/'\n",
        "train_path = path + 'train/'\n",
        "val_path = path + 'val/'\n",
        "\n",
        "train_set = maskedFaceDataset(train_path)\n",
        "val_set = maskedFaceDataset(val_path)\n",
        "\n",
        "num_workers = 2\n",
        "size_batch_train = 64\n",
        "size_batch_val = 2 * size_batch_train\n",
        "\n",
        "loader_train = DataLoader(train_set, batch_size=size_batch_train, \n",
        "                                           shuffle=True, \n",
        "                                           pin_memory=True, \n",
        "                                           num_workers=num_workers)\n",
        "\n",
        "loader_val = DataLoader(val_set, batch_size=size_batch_val, \n",
        "                                         shuffle=False,\n",
        "                                         num_workers=num_workers)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 0:   0%|          | 0/38 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 244, 244])\n",
            "2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 0:   0%|          | 0/38 [00:05<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "epoch = 0\n",
        "EPOCH = 1\n",
        "for epoch in range(EPOCH):\n",
        "\n",
        "    data_bar = tqdm(loader_train, desc=f\"Train Epoch {epoch}\")\n",
        "\n",
        "    for samples in data_bar:\n",
        "        print(samples[0].size())\n",
        "        print(len(samples))\n",
        "        img_1 = samples[0]\n",
        "        img_2 = samples[1]\n",
        "        \n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQyi4-icuQ7Y"
      },
      "outputs": [],
      "source": [
        "\"\"\"Train = 2397\n",
        "val = 300\n",
        "Test = 300\n",
        "\"\"\"\n",
        "\n",
        "N_IDENTITY = 8631\n",
        "PATH = 'C:/Users/Mattia/Documents/GitHub/masked-face/'\n",
        "model = resnet50(num_classes=N_IDENTITY)#False)\n",
        "load_state_dict(model, PATH+'weights/resnet50_ft_weight.pkl')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device=device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucerIxftuQ7a"
      },
      "source": [
        "**PARSER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement type (from versions: none)\n",
            "ERROR: No matching distribution found for type\n",
            "WARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\Mattia\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "n14f6iciuQ7c"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_args_parser():\n",
        "    \n",
        "    parser = argparse.ArgumentParser(description='Barlow Twins Training')\n",
        "    #parser.add_argument('data', type=Path, metavar='DIR',\n",
        "    #                help='path to dataset')\n",
        "    parser.add_argument('--workers', default=8, type=int, metavar='N',\n",
        "                    help='number of data loader workers')\n",
        "    parser.add_argument('--epochs', default=1000, type=int, metavar='N',\n",
        "                    help='number of total epochs to run')\n",
        "    parser.add_argument('--batch-size', default=2048, type=int, metavar='N',\n",
        "                    help='mini-batch size')\n",
        "    parser.add_argument('--learning-rate-weights', default=0.2, type=float, metavar='LR',\n",
        "                    help='base learning rate for weights')\n",
        "    parser.add_argument('--learning-rate-biases', default=0.0048, type=float, metavar='LR',\n",
        "                    help='base learning rate for biases and batch norm parameters')\n",
        "    parser.add_argument('--weight-decay', default=1e-6, type=float, metavar='W',\n",
        "                    help='weight decay')\n",
        "    parser.add_argument('--lambd', default=0.0051, type=float, metavar='L',\n",
        "                    help='weight on off-diagonal terms')\n",
        "    parser.add_argument('--projector', default='8192-8192-8192', type=str)\n",
        "                    #metavar='MLP', help='projector MLP')\n",
        "    parser.add_argument('--print-freq', default=100, type=int, metavar='N',\n",
        "                    help='print frequency')\n",
        "    parser.add_argument('--checkpoint-dir', default='./checkpoint/', type=Path,\n",
        "                    metavar='DIR', help='path to checkpoint directory')\n",
        "    parser.add_argument('--backbone_lr', default=0, type=float, \n",
        "                    help='Learning rate used for fine tuning the backbone, disabled by default')\n",
        "\n",
        "    return parser\n",
        "\n",
        "\n",
        "def main_worker(args):\n",
        "    PATH = 'C:/Users/Mattia/Documents/GitHub/masked-face/'\n",
        "    \n",
        "    model = BarlowTwins(args)\n",
        "    load_state_dict(model, PATH+'weights/resnet50_ft_weight.pkl')\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device=device)\n",
        "\n",
        "    param_weights = []\n",
        "    param_biases = []\n",
        "    for param in model.parameters():\n",
        "        if param.ndim == 1:\n",
        "            param_biases.append(param)\n",
        "        else:\n",
        "            param_weights.append(param)\n",
        "\n",
        "    parameters = [{'params': param_weights}, {'params': param_biases}]\n",
        "\n",
        "    optimizer = LARS(parameters, lr=0.001, weight_decay=args.weight_decay,\n",
        "                     weight_decay_filter=True,\n",
        "                     lars_adaptation_filter=True)\n",
        "\n",
        "    # automatically resume from checkpoint if it exists\n",
        "    if (args.checkpoint_dir / 'checkpoint.pth').is_file():\n",
        "        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',\n",
        "                          map_location='cpu')\n",
        "        start_epoch = ckpt['epoch']\n",
        "        model.load_state_dict(ckpt['model'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    step = 0\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        #for step, ((y1, y2), _) in enumerate(loader_train, start=epoch * len(loader_train)):\n",
        "        data_bar = tqdm(loader_train, desc=f\"Train Epoch {epoch}\")\n",
        "        for img_1, img_2 in data_bar:\n",
        "            step += 1\n",
        "            #y1 = y1.cuda(gpu, non_blocking=True)\n",
        "            #y2 = y2.cuda(gpu, non_blocking=True)\n",
        "            adjust_learning_rate(args, optimizer, loader_train, step)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                loss = model.forward(img_1, img_2)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            if step % args.print_freq == 0:\n",
        "                if args.rank == 0:\n",
        "                    stats = dict(epoch=epoch, step=step,\n",
        "                                 lr_weights=optimizer.param_groups[0]['lr'],\n",
        "                                 lr_biases=optimizer.param_groups[1]['lr'],\n",
        "                                 loss=loss.item(),\n",
        "                                 time=int(time.time() - start_time))\n",
        "                    print(json.dumps(stats))\n",
        "        if args.rank == 0:\n",
        "            # save checkpoint\n",
        "            state = dict(epoch=epoch + 1, model=model.state_dict(),\n",
        "                         optimizer=optimizer.state_dict())\n",
        "            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')\n",
        "    if args.rank == 0:\n",
        "        # save final model\n",
        "        torch.save(model.module.backbone.state_dict(),\n",
        "                   args.checkpoint_dir / 'resnet50.pth')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Mattia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
            "Train Epoch 0:   0%|          | 0/38 [00:00<?, ?it/s]C:\\Users\\Mattia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
          ]
        }
      ],
      "source": [
        "from barlowTwins import BarlowTwins\n",
        "\n",
        "PATH = 'C:/Users/Mattia/Documents/GitHub/masked-face/'\n",
        "args_parser = get_args_parser()\n",
        "args = args_parser.parse_args([])\n",
        "\n",
        "model = BarlowTwins(args)\n",
        "load_state_dict(model, PATH+'weights/resnet50_ft_weight.pkl')\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model.to(device=device)\n",
        "\n",
        "param_weights = []\n",
        "param_biases = []\n",
        "for param in model.parameters():\n",
        "        if param.ndim == 1:\n",
        "            param_biases.append(param)\n",
        "        else:\n",
        "            param_weights.append(param)\n",
        "\n",
        "parameters = [{'params': param_weights}, {'params': param_biases}]\n",
        "    \n",
        "optimizer = LARS(parameters, lr=0.001, weight_decay=args.weight_decay,\n",
        "                     weight_decay_filter=True,\n",
        "                     lars_adaptation_filter=True)\n",
        "\n",
        "    # automatically resume from checkpoint if it exists\n",
        "if (args.checkpoint_dir / 'checkpoint.pth').is_file():\n",
        "        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',\n",
        "                          map_location='cpu')\n",
        "        start_epoch = ckpt['epoch']\n",
        "        model.load_state_dict(ckpt['model'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer'])\n",
        "else:\n",
        "        start_epoch = 0\n",
        "\n",
        "start_time = time.time()\n",
        "#scaler = torch.cuda.amp.GradScaler()\n",
        "step = 0\n",
        "for epoch in range(start_epoch, args.epochs):\n",
        "        #for step, ((y1, y2), _) in enumerate(loader_train, start=epoch * len(loader_train)):\n",
        "        data_bar = tqdm(loader_train, desc=f\"Train Epoch {epoch}\")\n",
        "        for img_1, img_2 in data_bar:\n",
        "            step += 1\n",
        "            #y1 = y1.cuda(gpu, non_blocking=True)\n",
        "            #y2 = y2.cuda(gpu, non_blocking=True)\n",
        "            adjust_learning_rate(args, optimizer, loader_train, step)\n",
        "            optimizer.zero_grad()\n",
        "            #with torch.cuda.amp.autocast():\n",
        "            loss = model.forward(img_1, img_2)\n",
        "            #scaler.scale(loss).backward()\n",
        "            loss.backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            if step % args.print_freq == 0:\n",
        "                if args.rank == 0:\n",
        "                    stats = dict(epoch=epoch, step=step,\n",
        "                                 lr_weights=optimizer.param_groups[0]['lr'],\n",
        "                                 lr_biases=optimizer.param_groups[1]['lr'],\n",
        "                                 loss=loss.item(),\n",
        "                                 time=int(time.time() - start_time))\n",
        "                    print(json.dumps(stats))\n",
        "        if args.rank == 0:\n",
        "            # save checkpoint\n",
        "            state = dict(epoch=epoch + 1, model=model.state_dict(),\n",
        "                         optimizer=optimizer.state_dict())\n",
        "            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')\n",
        "if args.rank == 0:\n",
        "        # save final model\n",
        "        torch.save(model.module.backbone.state_dict(),\n",
        "                   args.checkpoint_dir / 'resnet50.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for ResNet:\n\tUnexpected key(s) in state_dict: \"fc.weight\", \"fc.bias\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Mattia\\Documents\\GitHub\\masked-face\\finetuning.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000016?line=2'>3</a>\u001b[0m       obj \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000016?line=4'>5</a>\u001b[0m weights \u001b[39m=\u001b[39m {key: torch\u001b[39m.\u001b[39mfrom_numpy(arr) \u001b[39mfor\u001b[39;00m key, arr \u001b[39min\u001b[39;00m pickle\u001b[39m.\u001b[39mloads(obj, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000016?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mbackbone\u001b[39m.\u001b[39;49mload_state_dict(weights)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1476'>1477</a>\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1477'>1478</a>\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1478'>1479</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1480'>1481</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1481'>1482</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1482'>1483</a>\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1483'>1484</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tUnexpected key(s) in state_dict: \"fc.weight\", \"fc.bias\". "
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "with open(PATH+'weights/resnet50_ft_weight.pkl', 'rb') as f:  \n",
        "      obj = f.read()\n",
        "\n",
        "weights = {key: torch.from_numpy(arr) for key, arr in pickle.loads(obj, encoding='latin1').items()}\n",
        "model.backbone.load_state_dict(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "args_parser = get_args_parser()\n",
        "args = args_parser.parse_args([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "bWsgh0RAuQ7f"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'unexpected key \"layer4.1.bn3.bias\" in state_dict'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Mattia\\Documents\\GitHub\\masked-face\\finetuning.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000007?line=0'>1</a>\u001b[0m main_worker(args)\n",
            "\u001b[1;32mc:\\Users\\Mattia\\Documents\\GitHub\\masked-face\\finetuning.ipynb Cell 7'\u001b[0m in \u001b[0;36mmain_worker\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000006?line=32'>33</a>\u001b[0m PATH \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:/Users/Mattia/Documents/GitHub/masked-face/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000006?line=34'>35</a>\u001b[0m model \u001b[39m=\u001b[39m BarlowTwins(args)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000006?line=35'>36</a>\u001b[0m load_state_dict(model, PATH\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mweights/resnet50_ft_weight.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000006?line=36'>37</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000006?line=37'>38</a>\u001b[0m model\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\Mattia\\Documents\\GitHub\\masked-face\\utils.py:31\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(model, fname)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Mattia/Documents/GitHub/masked-face/utils.py?line=27'>28</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mWhile copying the parameter named \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, whose dimensions in the model are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and whose \u001b[39m\u001b[39m'\u001b[39m\\\n\u001b[0;32m     <a href='file:///c%3A/Users/Mattia/Documents/GitHub/masked-face/utils.py?line=28'>29</a>\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mdimensions in the checkpoint are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name, own_state[name]\u001b[39m.\u001b[39msize(), param\u001b[39m.\u001b[39msize()))\n\u001b[0;32m     <a href='file:///c%3A/Users/Mattia/Documents/GitHub/masked-face/utils.py?line=29'>30</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Mattia/Documents/GitHub/masked-face/utils.py?line=30'>31</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39munexpected key \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m in state_dict\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name))\n",
            "\u001b[1;31mKeyError\u001b[0m: 'unexpected key \"layer4.1.bn3.bias\" in state_dict'"
          ]
        }
      ],
      "source": [
        "main_worker(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "finetuning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
