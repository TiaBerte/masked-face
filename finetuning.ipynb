{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhu-IvtmuU97"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TiaBerte/masked-face/blob/main/finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sudo apt-get install git-lfs\n",
        "!git-lfs clone https://github.com/TiaBerte/masked-face.git\n",
        "%cd masked-face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H7yNytZAuQ7T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from barlowTwins import BarlowTwins\n",
        "from utils import adjust_learning_rate, load_state_dict, get_mean_and_std, LARS\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataload import maskedFaceDataset\n",
        "import argparse\n",
        "\n",
        "path = './dataset/'\n",
        "train_path = path + 'train/'\n",
        "val_path = path + 'val/'\n",
        "\n",
        "train_set = maskedFaceDataset(train_path)\n",
        "val_set = maskedFaceDataset(val_path)\n",
        "\n",
        "num_workers = 2\n",
        "size_batch_train = 64\n",
        "size_batch_val = 2 * size_batch_train\n",
        "\n",
        "loader_train = DataLoader(train_set, batch_size=size_batch_train, \n",
        "                                           shuffle=True, \n",
        "                                           pin_memory=True, \n",
        "                                           num_workers=num_workers)\n",
        "\n",
        "loader_val = DataLoader(val_set, batch_size=size_batch_val, \n",
        "                                         shuffle=False,\n",
        "                                         num_workers=num_workers)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 0:   0%|          | 0/38 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/omar/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/omar/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/omar/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/omar/VSCode/masked-face/masked-face/dataload.py\", line 32, in __getitem__\n    img_sampled = random.sample(img_list, 2)\n  File \"/usr/lib/python3.8/random.py\", line 363, in sample\n    raise ValueError(\"Sample larger than population or is negative\")\nValueError: Sample larger than population or is negative\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/omar/VSCode/masked-face/masked-face/finetuning.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/omar/VSCode/masked-face/masked-face/finetuning.ipynb#ch0000003?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCH):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/omar/VSCode/masked-face/masked-face/finetuning.ipynb#ch0000003?line=6'>7</a>\u001b[0m     data_bar \u001b[39m=\u001b[39m tqdm(loader_train, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/omar/VSCode/masked-face/masked-face/finetuning.ipynb#ch0000003?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m data_bar:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omar/VSCode/masked-face/masked-face/finetuning.ipynb#ch0000003?line=9'>10</a>\u001b[0m         \u001b[39mprint\u001b[39m(samples[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omar/VSCode/masked-face/masked-face/finetuning.ipynb#ch0000003?line=10'>11</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(samples))\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:1180\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/omar/.local/lib/python3.8/site-packages/tqdm/std.py?line=1176'>1177</a>\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   <a href='file:///home/omar/.local/lib/python3.8/site-packages/tqdm/std.py?line=1178'>1179</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/omar/.local/lib/python3.8/site-packages/tqdm/std.py?line=1179'>1180</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   <a href='file:///home/omar/.local/lib/python3.8/site-packages/tqdm/std.py?line=1180'>1181</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   <a href='file:///home/omar/.local/lib/python3.8/site-packages/tqdm/std.py?line=1181'>1182</a>\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/omar/.local/lib/python3.8/site-packages/tqdm/std.py?line=1182'>1183</a>\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1221'>1222</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1222'>1223</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1223'>1224</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1247'>1248</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1248'>1249</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1249'>1250</a>\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=1250'>1251</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/_utils.py?line=452'>453</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/_utils.py?line=453'>454</a>\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/_utils.py?line=454'>455</a>\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/_utils.py?line=455'>456</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///home/omar/.local/lib/python3.8/site-packages/torch/_utils.py?line=456'>457</a>\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
            "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/omar/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/omar/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/omar/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/omar/VSCode/masked-face/masked-face/dataload.py\", line 32, in __getitem__\n    img_sampled = random.sample(img_list, 2)\n  File \"/usr/lib/python3.8/random.py\", line 363, in sample\n    raise ValueError(\"Sample larger than population or is negative\")\nValueError: Sample larger than population or is negative\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "epoch = 0\n",
        "EPOCH = 1\n",
        "for epoch in range(EPOCH):\n",
        "\n",
        "    data_bar = tqdm(loader_train, desc=f\"Train Epoch {epoch}\")\n",
        "\n",
        "    for samples in data_bar:\n",
        "        print(samples[0].size())\n",
        "        print(len(samples))\n",
        "        img_1 = samples[0]\n",
        "        img_2 = samples[1]\n",
        "        \n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZQyi4-icuQ7Y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=8631, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Train = 2397\n",
        "val = 300\n",
        "Test = 300\n",
        "\"\"\"\n",
        "\n",
        "N_IDENTITY = 8631\n",
        "PATH = './'\n",
        "model = resnet50(num_classes=N_IDENTITY)#False)\n",
        "load_state_dict(model, PATH+'weights/resnet50_ft_weight.pkl')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device=device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucerIxftuQ7a"
      },
      "source": [
        "**PARSER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement type (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for type\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n14f6iciuQ7c"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def get_args_parser():\n",
        "    \n",
        "    parser = argparse.ArgumentParser(description='Barlow Twins Training')\n",
        "    #parser.add_argument('data', type=Path, metavar='DIR',\n",
        "    #                help='path to dataset')\n",
        "    parser.add_argument('--workers', default=8, type=int, metavar='N',\n",
        "                    help='number of data loader workers')\n",
        "    parser.add_argument('--epochs', default=1000, type=int, metavar='N',\n",
        "                    help='number of total epochs to run')\n",
        "    parser.add_argument('--batch-size', default=2048, type=int, metavar='N',\n",
        "                    help='mini-batch size')\n",
        "    parser.add_argument('--learning-rate-weights', default=0.2, type=float, metavar='LR',\n",
        "                    help='base learning rate for weights')\n",
        "    parser.add_argument('--learning-rate-biases', default=0.0048, type=float, metavar='LR',\n",
        "                    help='base learning rate for biases and batch norm parameters')\n",
        "    parser.add_argument('--weight-decay', default=1e-6, type=float, metavar='W',\n",
        "                    help='weight decay')\n",
        "    parser.add_argument('--lambd', default=0.0051, type=float, metavar='L',\n",
        "                    help='weight on off-diagonal terms')\n",
        "    parser.add_argument('--projector', default='8192-8192-8192', type=str)\n",
        "                    #metavar='MLP', help='projector MLP')\n",
        "    parser.add_argument('--print-freq', default=100, type=int, metavar='N',\n",
        "                    help='print frequency')\n",
        "    parser.add_argument('--checkpoint-dir', default='./checkpoint/', type=Path,\n",
        "                    metavar='DIR', help='path to checkpoint directory')\n",
        "    parser.add_argument('--backbone_lr', default=0, type=float, \n",
        "                    help='Learning rate used for fine tuning the backbone, disabled by default')\n",
        "\n",
        "    return parser\n",
        "\n",
        "\n",
        "def main_worker(args):\n",
        "    PATH = './'\n",
        "    \n",
        "    model = BarlowTwins(args)\n",
        "    load_state_dict(model, PATH+'weights/resnet50_ft_weight.pkl')\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device=device)\n",
        "\n",
        "    param_weights = []\n",
        "    param_biases = []\n",
        "    for param in model.parameters():\n",
        "        if param.ndim == 1:\n",
        "            param_biases.append(param)\n",
        "        else:\n",
        "            param_weights.append(param)\n",
        "\n",
        "    parameters = [{'params': param_weights}, {'params': param_biases}]\n",
        "\n",
        "    optimizer = LARS(parameters, lr=0.001, weight_decay=args.weight_decay,\n",
        "                     weight_decay_filter=True,\n",
        "                     lars_adaptation_filter=True)\n",
        "\n",
        "    # automatically resume from checkpoint if it exists\n",
        "    if (args.checkpoint_dir / 'checkpoint.pth').is_file():\n",
        "        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',\n",
        "                          map_location='cpu')\n",
        "        start_epoch = ckpt['epoch']\n",
        "        model.load_state_dict(ckpt['model'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    step = 0\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        #for step, ((y1, y2), _) in enumerate(loader_train, start=epoch * len(loader_train)):\n",
        "        data_bar = tqdm(loader_train, desc=f\"Train Epoch {epoch}\")\n",
        "        for img_1, img_2 in data_bar:\n",
        "            step += 1\n",
        "            #y1 = y1.cuda(gpu, non_blocking=True)\n",
        "            #y2 = y2.cuda(gpu, non_blocking=True)\n",
        "            adjust_learning_rate(args, optimizer, loader_train, step)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                loss = model.forward(img_1, img_2)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            if step % args.print_freq == 0:\n",
        "                if args.rank == 0:\n",
        "                    stats = dict(epoch=epoch, step=step,\n",
        "                                 lr_weights=optimizer.param_groups[0]['lr'],\n",
        "                                 lr_biases=optimizer.param_groups[1]['lr'],\n",
        "                                 loss=loss.item(),\n",
        "                                 time=int(time.time() - start_time))\n",
        "                    print(json.dumps(stats))\n",
        "        if args.rank == 0:\n",
        "            # save checkpoint\n",
        "            state = dict(epoch=epoch + 1, model=model.state_dict(),\n",
        "                         optimizer=optimizer.state_dict())\n",
        "            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')\n",
        "    if args.rank == 0:\n",
        "        # save final model\n",
        "        torch.save(model.module.backbone.state_dict(),\n",
        "                   args.checkpoint_dir / 'resnet50.pth')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train Epoch 0:   0%|          | 0/38 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "from barlowTwins import BarlowTwins\n",
        "\n",
        "PATH = './'\n",
        "args_parser = get_args_parser()\n",
        "args = args_parser.parse_args([])\n",
        "\n",
        "model = BarlowTwins(args)\n",
        "load_state_dict(model, PATH+'weights/resnet50_ft_weight.pkl')\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model.to(device=device)\n",
        "\n",
        "param_weights = []\n",
        "param_biases = []\n",
        "for param in model.parameters():\n",
        "        if param.ndim == 1:\n",
        "            param_biases.append(param)\n",
        "        else:\n",
        "            param_weights.append(param)\n",
        "\n",
        "parameters = [{'params': param_weights}, {'params': param_biases}]\n",
        "    \n",
        "optimizer = LARS(parameters, lr=0.001, weight_decay=args.weight_decay,\n",
        "                     weight_decay_filter=True,\n",
        "                     lars_adaptation_filter=True)\n",
        "\n",
        "    # automatically resume from checkpoint if it exists\n",
        "if (args.checkpoint_dir / 'checkpoint.pth').is_file():\n",
        "        ckpt = torch.load(args.checkpoint_dir / 'checkpoint.pth',\n",
        "                          map_location='cpu')\n",
        "        start_epoch = ckpt['epoch']\n",
        "        model.load_state_dict(ckpt['model'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer'])\n",
        "else:\n",
        "        start_epoch = 0\n",
        "\n",
        "start_time = time.time()\n",
        "#scaler = torch.cuda.amp.GradScaler()\n",
        "step = 0\n",
        "for epoch in range(start_epoch, args.epochs):\n",
        "        #for step, ((y1, y2), _) in enumerate(loader_train, start=epoch * len(loader_train)):\n",
        "        data_bar = tqdm(loader_train, desc=f\"Train Epoch {epoch}\")\n",
        "        for img_1, img_2 in data_bar:\n",
        "            step += 1\n",
        "            #y1 = y1.cuda(gpu, non_blocking=True)\n",
        "            #y2 = y2.cuda(gpu, non_blocking=True)\n",
        "            adjust_learning_rate(args, optimizer, loader_train, step)\n",
        "            optimizer.zero_grad()\n",
        "            #with torch.cuda.amp.autocast():\n",
        "            loss = model.forward(img_1, img_2)\n",
        "            #scaler.scale(loss).backward()\n",
        "            loss.backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            if step % args.print_freq == 0:\n",
        "                if args.rank == 0:\n",
        "                    stats = dict(epoch=epoch, step=step,\n",
        "                                 lr_weights=optimizer.param_groups[0]['lr'],\n",
        "                                 lr_biases=optimizer.param_groups[1]['lr'],\n",
        "                                 loss=loss.item(),\n",
        "                                 time=int(time.time() - start_time))\n",
        "                    print(json.dumps(stats))\n",
        "        if args.rank == 0:\n",
        "            # save checkpoint\n",
        "            state = dict(epoch=epoch + 1, model=model.state_dict(),\n",
        "                         optimizer=optimizer.state_dict())\n",
        "            torch.save(state, args.checkpoint_dir / 'checkpoint.pth')\n",
        "if args.rank == 0:\n",
        "        # save final model\n",
        "        torch.save(model.module.backbone.state_dict(),\n",
        "                   args.checkpoint_dir / 'resnet50.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for ResNet:\n\tUnexpected key(s) in state_dict: \"fc.weight\", \"fc.bias\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Mattia\\Documents\\GitHub\\masked-face\\finetuning.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000016?line=2'>3</a>\u001b[0m       obj \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000016?line=4'>5</a>\u001b[0m weights \u001b[39m=\u001b[39m {key: torch\u001b[39m.\u001b[39mfrom_numpy(arr) \u001b[39mfor\u001b[39;00m key, arr \u001b[39min\u001b[39;00m pickle\u001b[39m.\u001b[39mloads(obj, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000016?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mbackbone\u001b[39m.\u001b[39;49mload_state_dict(weights)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1476'>1477</a>\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1477'>1478</a>\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1478'>1479</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1480'>1481</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1481'>1482</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1482'>1483</a>\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   <a href='file:///c%3A/Users/Mattia/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=1483'>1484</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tUnexpected key(s) in state_dict: \"fc.weight\", \"fc.bias\". "
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "with open(PATH+'weights/resnet50_ft_weight.pkl', 'rb') as f:  \n",
        "      obj = f.read()\n",
        "\n",
        "weights = {key: torch.from_numpy(arr) for key, arr in pickle.loads(obj, encoding='latin1').items()}\n",
        "model.backbone.load_state_dict(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "args_parser = get_args_parser()\n",
        "args = args_parser.parse_args([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "bWsgh0RAuQ7f"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'unexpected key \"layer4.1.bn3.bias\" in state_dict'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Mattia\\Documents\\GitHub\\masked-face\\finetuning.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000007?line=0'>1</a>\u001b[0m main_worker(args)\n",
            "\u001b[1;32mc:\\Users\\Mattia\\Documents\\GitHub\\masked-face\\finetuning.ipynb Cell 7'\u001b[0m in \u001b[0;36mmain_worker\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000006?line=32'>33</a>\u001b[0m PATH \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:/Users/Mattia/Documents/GitHub/masked-face/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000006?line=34'>35</a>\u001b[0m model \u001b[39m=\u001b[39m BarlowTwins(args)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000006?line=35'>36</a>\u001b[0m load_state_dict(model, PATH\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mweights/resnet50_ft_weight.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000006?line=36'>37</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mattia/Documents/GitHub/masked-face/finetuning.ipynb#ch0000006?line=37'>38</a>\u001b[0m model\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\Mattia\\Documents\\GitHub\\masked-face\\utils.py:31\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(model, fname)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Mattia/Documents/GitHub/masked-face/utils.py?line=27'>28</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mWhile copying the parameter named \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, whose dimensions in the model are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and whose \u001b[39m\u001b[39m'\u001b[39m\\\n\u001b[0;32m     <a href='file:///c%3A/Users/Mattia/Documents/GitHub/masked-face/utils.py?line=28'>29</a>\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mdimensions in the checkpoint are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name, own_state[name]\u001b[39m.\u001b[39msize(), param\u001b[39m.\u001b[39msize()))\n\u001b[0;32m     <a href='file:///c%3A/Users/Mattia/Documents/GitHub/masked-face/utils.py?line=29'>30</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Mattia/Documents/GitHub/masked-face/utils.py?line=30'>31</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39munexpected key \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m in state_dict\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name))\n",
            "\u001b[1;31mKeyError\u001b[0m: 'unexpected key \"layer4.1.bn3.bias\" in state_dict'"
          ]
        }
      ],
      "source": [
        "main_worker(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "finetuning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
